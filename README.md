# Project-2
By employing knowledge distillation, the focus is indeed on training a smaller student CNN model to  match or surpass the accuracy of a larger teacher CNN model.
